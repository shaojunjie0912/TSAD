import torch
import torch.nn as nn

# 输入数据的形状为 (batch_size, seq_len, num_features)
# 层归一化作用于 **每个样本的每个时间步的特征维度**​ (即最后一个维度 num_features)

# - ​计算均值和方差：对于每个样本的每个时间步，独立计算该时间步所有特征的均值和方差。
# ​- 归一化与缩放/平移：使用上述统计量对特征进行标准化 (均值为0，方差为1)
#   然后通过可学习的参数 gamma (缩放) 和 beta (平移) 调整数值范围。

# 例如，输入为 (32, 100, 10) 时，层归一化会对 32 个样本中的每个样本的100个时间步分别处理
# 每个时间步的10个特征会被独立归一化。

# 模拟输入：[batch_size, seq_len, feature_dim]
x = torch.tensor(
    [
        [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]],
        [[13.0, 14.0, 15.0], [16.0, 17.0, 18.0], [19.0, 20.0, 21.0], [22.0, 23.0, 24.0]],
    ]
)  # shape: [2, 4, 3]

print("原始数据形状:", x.shape)

# 对最后一个维度做 LayerNorm（即每个时间步的3个特征做归一化）
layer_norm = nn.LayerNorm(normalized_shape=3)

# 应用归一化
out = layer_norm(x)

print("归一化后的输出:")
print(out)
