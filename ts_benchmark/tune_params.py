import argparse
import copy
import os
from typing import Any, Dict, List

import numpy as np
import optuna
import pandas as pd
import tomli
import tomli_w
from baselines.swift.swift_pipeline import swift_find_anomalies
from evaluation.metrics.anomaly_detection_metrics_label import affiliation_f
from tools.tools import set_seed


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½ TOML é…ç½®æ–‡ä»¶ã€‚"""
    with open(config_path, "rb") as f:
        return tomli.load(f)


def update_config(base_config: Dict[str, Any], param_updates: Dict[str, Any]) -> Dict[str, Any]:
    """æ ¹æ® Optuna æä¾›çš„å‚æ•°æ›´æ–°é…ç½®å­—å…¸ã€‚"""
    config = copy.deepcopy(base_config)
    for key_path, value in param_updates.items():
        keys = key_path.split(".")
        current = config
        for key in keys[:-1]:
            current = current.get(key, {})
        current[keys[-1]] = value
    return config


def save_optimal_config(base_config: Dict[str, Any], best_params: Dict[str, Any], output_path: str):
    """å°†ä¼˜åŒ–åçš„é…ç½®ä¿å­˜åˆ° TOML æ–‡ä»¶ã€‚"""
    if not best_params:
        print("ğŸŸ¡ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆå‚æ•°ï¼Œä¸ä¿å­˜é…ç½®æ–‡ä»¶ã€‚")
        return

    optimal_config = update_config(base_config, best_params)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "wb") as f:
        tomli_w.dump(optimal_config, f)
    print(f"ğŸ’¾ æ–°çš„æœ€ä½³é…ç½®å·²ä¿å­˜åˆ°: {output_path}")


def objective(
    trial: optuna.trial.Trial, base_config: Dict[str, Any], data: np.ndarray, labels: np.ndarray
) -> float:
    """Optuna æ ¸å¿ƒç›®æ ‡å‡½æ•°"""
    param_updates = {}
    param_updates["trial_number"] = trial.number

    # ---- data æ•°æ®å¤„ç† ----
    seq_len = trial.suggest_categorical("data.seq_len", [64, 128, 256])

    possible_patch_sizes = [8, 16, 32]
    valid_patch_sizes = [p for p in possible_patch_sizes if p < seq_len]
    if not valid_patch_sizes:
        raise optuna.TrialPruned("No valid patch_size for the chosen seq_len.")
    patch_size = trial.suggest_categorical("data.patch_size", valid_patch_sizes)

    divisible_length = seq_len - patch_size

    # å¯»æ‰¾æ‰€æœ‰æœ‰æ•ˆçš„ patch_stride (å³ divisible_length çš„æ‰€æœ‰çº¦æ•°)
    valid_strides = []
    for i in range(1, int(divisible_length**0.5) + 1):
        if divisible_length % i == 0:
            valid_strides.append(i)
            if i * i != divisible_length:
                valid_strides.append(divisible_length // i)
    valid_strides.sort()

    practical_strides = [s for s in valid_strides if 4 <= s <= patch_size]
    if not practical_strides:
        if not valid_strides:
            raise optuna.TrialPruned("No valid strides found.")
        patch_stride = trial.suggest_categorical("data.patch_stride", valid_strides)
    else:
        patch_stride = trial.suggest_categorical("data.patch_stride", practical_strides)

    param_updates["data.seq_len"] = seq_len
    param_updates["data.patch_size"] = patch_size
    param_updates["data.patch_stride"] = patch_stride

    # ---- loss æŸå¤±å‡½æ•° ----
    param_updates["loss.ccd_loss_lambda"] = trial.suggest_float("loss.ccd_loss_lambda", 1e-4, 1e-1, log=True)
    param_updates["loss.scale_loss_lambda"] = trial.suggest_float("loss.scale_loss_lambda", 0.2, 1.0)
    param_updates["loss.ccd_regular_lambda"] = trial.suggest_float("loss.ccd_regular_lambda", 0.05, 0.5)
    param_updates["loss.ccd_align_lambda"] = trial.suggest_float("loss.ccd_align_lambda", 0.5, 2.0)

    # ---- model.FM å‰å‘æ¨¡å— ----
    param_updates["model.FM.level"] = trial.suggest_int("fm.level", 2, 5)
    param_updates["model.FM.wavelet"] = trial.suggest_categorical("fm.wavelet", ["db4", "sym4", "coif2"])

    # ---- model.CFM é€šé“èåˆæ¨¡å— ----
    param_updates["model.CFM.num_heads"] = trial.suggest_categorical("cfm.num_heads", [2, 4, 8])
    param_updates["model.CFM.attention_dropout"] = trial.suggest_float("cfm.attention_dropout", 0.05, 0.3)
    d_cf_val = trial.suggest_categorical("cfm.d_cf", [64, 96, 128])
    d_model_val = trial.suggest_categorical("cfm.d_model", [64, 96, 128])
    if d_model_val < d_cf_val:
        raise optuna.TrialPruned()
    param_updates["model.CFM.d_cf"] = d_cf_val
    param_updates["model.CFM.d_model"] = d_model_val
    param_updates["model.CFM.num_layers"] = trial.suggest_int("cfm.num_layers", 3, 6)
    param_updates["model.CFM.dropout"] = trial.suggest_float("cfm.dropout", 0.1, 0.25)
    param_updates["model.CFM.num_gat_heads"] = trial.suggest_categorical("cfm.num_gat_heads", [2, 4, 8])
    param_updates["model.CFM.gat_head_dim"] = trial.suggest_categorical("cfm.gat_head_dim", [8, 16, 32])

    # ---- model.TSRM æ—¶å°ºé‡æ„æ¨¡å— ----
    param_updates["model.TSRM.is_flatten_individual"] = trial.suggest_categorical(
        "tsrm.is_flatten_individual", [True, False]
    )

    # ---- training è®­ç»ƒé…ç½® ----
    param_updates["training.learning_rate"] = trial.suggest_float(
        "training.learning_rate", 1e-4, 5e-3, log=True
    )
    param_updates["training.batch_size"] = trial.suggest_categorical("training.batch_size", [32, 64])

    # ---- anomaly_detection å¼‚å¸¸æ£€æµ‹é…ç½® ----
    param_updates["anomaly_detection.scale_score_lambda"] = trial.suggest_float(
        "anomaly_detection.scale_score_lambda", 0.1, 1.0
    )

    # åˆ›å»ºå¹¶è¯„ä¼°å½“å‰è¯•éªŒçš„é…ç½®
    current_config = update_config(base_config, param_updates)

    try:
        predictions = swift_find_anomalies(data=data, config=current_config)
        aff_f1 = affiliation_f(labels, predictions)
        return aff_f1
    except Exception as e:
        print(f"Trial {trial.number} failed with an exception: {e}")
        raise optuna.TrialPruned()


def run_tuning_for_ratio(
    ratio: float, base_config: Dict[str, Any], data: np.ndarray, labels: np.ndarray, args: argparse.Namespace
) -> None:
    """ä¸ºæŒ‡å®šçš„å¼‚å¸¸ç‡è¿è¡Œè¶…å‚æ•°è°ƒä¼˜"""
    print(f"ğŸš€ ========================================================")
    print(f"ğŸš€ å¼€å§‹ä¸º Anomaly Ratio = {ratio}% è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")
    print(f"ğŸš€ ========================================================")

    # æ ¼å¼åŒ–å½“å‰å¼‚å¸¸ç‡çš„è·¯å¾„
    current_output_config = args.output_config.format(ratio=ratio)
    current_study_name = args.study_name.format(ratio=ratio)

    # åˆ›å»ºå½“å‰å¼‚å¸¸ç‡çš„é…ç½®
    current_config = copy.deepcopy(base_config)
    current_config["anomaly_detection"]["anomaly_ratio"] = ratio
    current_config["anomaly_detection"]["threshold_strategy"] = args.threshold_strategy
    current_config["anomaly_detection"]["aggregation_method"] = args.aggregation_method

    # åˆ›å»º Optuna ç ”ç©¶
    storage_name = f"sqlite:///{current_study_name}.db"
    study = optuna.create_study(
        study_name=current_study_name,
        storage=storage_name,
        direction="maximize",
        load_if_exists=True,
        sampler=optuna.samplers.TPESampler(),
        pruner=optuna.pruners.MedianPruner(),
    )

    # å®šä¹‰å›è°ƒå‡½æ•°ï¼Œç”¨äºå®æ—¶ä¿å­˜æœ€ä½³é…ç½®
    def save_best_callback(study: optuna.study.Study, trial: optuna.trial.FrozenTrial):
        if study.best_trial.number == trial.number:
            print(f"ğŸ‰ Trial {trial.number} å‘ç°æ–°çš„æœ€ä½³ Affiliation F1 åˆ†æ•°: {trial.value:.4f}")

            # é‡æ˜ å°„å‚æ•°é”®
            key_map = {
                "cfm.": "model.CFM.",
                "fm.": "model.FM.",
                "tsrm.": "model.TSRM.",
            }

            mapped_params = {}
            for k, v in trial.params.items():
                new_key = k
                for short, full in key_map.items():
                    if k.startswith(short):
                        new_key = k.replace(short, full)
                        break
                mapped_params[new_key] = v

            save_optimal_config(current_config, mapped_params, current_output_config)

    # åˆ›å»ºç›®æ ‡å‡½æ•°
    objective_func = lambda trial: objective(trial, current_config, data, labels)

    # è¿è¡Œä¼˜åŒ–
    study.optimize(objective_func, n_trials=args.n_trials, callbacks=[save_best_callback])

    print(f"âœ… Anomaly Ratio = {ratio}% çš„è°ƒä¼˜å®Œæˆã€‚æœ€ä½³é…ç½®å·²ä¿å­˜åˆ° {current_output_config}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ Optuna å¯¹ SWIFT è¿›è¡Œæ‰¹é‡è¶…å‚æ•°ä¼˜åŒ–", formatter_class=argparse.RawTextHelpFormatter
    )

    # å¿…éœ€å‚æ•°
    parser.add_argument("--task-name", type=str, required=True, help="ä»»åŠ¡åç§° (å¦‚: find_anomalies)")
    parser.add_argument("--dataset-name", type=str, required=True, help="æ•°æ®é›†åç§° (å¦‚: ASD_dataset_1)")
    parser.add_argument("--algorithm-name", type=str, required=True, help="å¼‚å¸¸æ£€æµ‹ç®—æ³•åç§° (å¦‚: swift)")

    parser.add_argument(
        "--base-config",
        type=str,
        default="configs/{task_name}/{dataset_name}/{algorithm_name}_base.toml",
        help="åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    # è¾“å‡ºé…ç½®
    parser.add_argument(
        "--output-config",
        type=str,
        default="configs/{task_name}/{dataset_name}/{algorithm_name}_best_ar_{ratio}.toml",
        help="è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--dataset_path",
        type=str,
        default="datasets/{dataset_name}.csv",
        help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (ç”¨äºè®­ç»ƒ+éªŒè¯)",
    )

    # å¼‚å¸¸ç‡é…ç½®
    parser.add_argument(
        "--anomaly-ratios", type=str, default="1,3,5,8", help="è¦è°ƒä¼˜çš„å¼‚å¸¸ç‡åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš” (é»˜è®¤: 1,3,5,8)"
    )

    # å›ºå®šç­–ç•¥å‚æ•°
    parser.add_argument(
        "--threshold-strategy", type=str, default="adaptive", help="é˜ˆå€¼è®¡ç®—ç­–ç•¥ (é»˜è®¤: adaptive)"
    )
    parser.add_argument(
        "--aggregation-method", type=str, default="weighted_max", help="åˆ†æ•°èšåˆæ–¹æ³• (é»˜è®¤: weighted_max)"
    )

    # è°ƒä¼˜é…ç½®
    parser.add_argument("--n-trials", type=int, default=100, help="æ¯ä¸ªå¼‚å¸¸ç‡çš„è¯•éªŒæ¬¡æ•° (é»˜è®¤: 100)")

    parser.add_argument(
        "--study-name",
        type=str,
        default="{task_name}-{dataset_name}-{algorithm_name}-ar{ratio}",
        help="Optuna ç ”ç©¶åç§°",
    )

    # å¯é€‰å‚æ•°
    parser.add_argument("--seed", type=int, default=1037, help="éšæœºç§å­ (é»˜è®¤: 1037)")

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)

    # æ ¼å¼åŒ–è·¯å¾„æ¨¡æ¿
    args.base_config = args.base_config.format(
        task_name=args.task_name, dataset_name=args.dataset_name, algorithm_name=args.algorithm_name
    )

    args.output_config = args.output_config.format(
        task_name=args.task_name,
        dataset_name=args.dataset_name,
        algorithm_name=args.algorithm_name,
        ratio="{ratio}",  # ä¿ç•™ ratio å ä½ç¬¦ï¼Œç¨ååœ¨ run_tuning_for_ratio ä¸­æ ¼å¼åŒ–
    )

    args.dataset_path = args.dataset_path.format(dataset_name=args.dataset_name)

    args.study_name = args.study_name.format(
        task_name=args.task_name,
        dataset_name=args.dataset_name,
        algorithm_name=args.algorithm_name,
        ratio="{ratio}",  # ä¿ç•™ ratio å ä½ç¬¦ï¼Œç¨ååœ¨ run_tuning_for_ratio ä¸­æ ¼å¼åŒ–
    )

    # è§£æå¼‚å¸¸ç‡åˆ—è¡¨
    try:
        anomaly_ratios = [float(x.strip()) for x in args.anomaly_ratios.split(",")]
    except ValueError:
        print("âŒ å¼‚å¸¸ç‡æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨é€—å·åˆ†éš”çš„æ•°å­—åˆ—è¡¨")

    print(f"ğŸš€ å‚æ•°è‡ªåŠ¨è°ƒä¼˜å¼€å§‹...")
    print(f"ğŸ¯ ä»»åŠ¡: {args.task_name} | æ•°æ®é›†: {args.dataset_name} | ç®—æ³•: {args.algorithm_name.upper()}")
    print(f"ğŸ”¬ å¼‚å¸¸ç‡: {anomaly_ratios}")
    print(f"ğŸ”¬ æ¯ä¸ªå¼‚å¸¸ç‡è¯•éªŒæ¬¡æ•°: {args.n_trials}")
    print(f"ğŸ›ï¸ é˜ˆå€¼ç­–ç•¥: {args.threshold_strategy}")
    print(f"ğŸ”— èšåˆæ–¹æ³•: {args.aggregation_method}")

    # åŠ è½½åŸºç¡€é…ç½®å’Œæ•°æ®
    base_config = load_config(args.base_config)
    df = pd.read_csv(args.dataset_path)
    data = df.iloc[:, :-1].values
    labels = df.iloc[:, -1].to_numpy()

    print(f"ğŸ“ æ•°æ®å½¢çŠ¶: {data.shape}, æ ‡ç­¾å½¢çŠ¶: {labels.shape}")

    # ä¾æ¬¡ä¸ºæ¯ä¸ªå¼‚å¸¸ç‡è¿è¡Œè°ƒä¼˜
    for i, ratio in enumerate(anomaly_ratios, 1):
        print(f"\n[{i}/{len(anomaly_ratios)}] å¤„ç†å¼‚å¸¸ç‡: {ratio}%")
        try:
            run_tuning_for_ratio(ratio, base_config, data, labels, args)
        except Exception as e:
            print(f"âŒ å¼‚å¸¸ç‡ {ratio}% è°ƒä¼˜å¤±è´¥: {e}")
            continue

    print(f"\nğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰ {len(anomaly_ratios)} ä¸ªå¼‚å¸¸ç‡çš„è°ƒä¼˜ä»»åŠ¡å‡å·²æ‰§è¡Œå®Œæ¯•! ğŸ‰ğŸ‰ğŸ‰")
