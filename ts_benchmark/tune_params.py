#!/usr/bin/env python3

"""
SWIFT å‚æ•°è‡ªåŠ¨è°ƒä¼˜è„šæœ¬
æ¯ä¸ªé…ç½®å®Œæˆåç«‹å³ä¿å­˜æœ€ä½³ç»“æœï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
ä½¿ç”¨ tomli è¯»å– TOMLï¼Œtomli-w å†™å…¥ TOML
"""

import copy
import itertools
import json
import os
from typing import Any, Dict, List

import numpy as np
import pandas as pd
import tomli
import tomli_w
from baselines.swift.swift_pipeline import swift_find_anomalies
from evaluation.metrics.anomaly_detection_metrics_label import affiliation_f
from tools.tools import set_seed


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, "rb") as f:
        return tomli.load(f)


def update_config(base_config: Dict[str, Any], param_updates: Dict[str, Any]) -> Dict[str, Any]:
    """æ›´æ–°é…ç½®å‚æ•°"""
    config = copy.deepcopy(base_config)

    for key_path, value in param_updates.items():
        keys = key_path.split(".")
        current = config
        for key in keys[:-1]:
            current = current[key]
        current[keys[-1]] = value

    return config


def evaluate_config(data: np.ndarray, labels: np.ndarray, config: Dict[str, Any]) -> float | None:
    """è¯„ä¼°å•ä¸ªé…ç½®çš„æ€§èƒ½"""
    try:
        predictions = swift_find_anomalies(data=data, config=config)
        score = affiliation_f(labels, predictions)
        return score
    except Exception as e:
        print(f"é…ç½®è¯„ä¼°å¤±è´¥: {e}")
        return None


def save_progress(progress_file: str, results: List[Dict], best_params: Dict, best_score: float):
    """ä¿å­˜æœç´¢è¿›åº¦"""
    progress_data = {
        "results": results,
        "best_params": best_params,
        "best_score": best_score,
        "completed_configs": len(results),
    }
    temp_file = progress_file + ".tmp"
    with open(temp_file, "w") as f:
        json.dump(progress_data, f, indent=2)
    # å†™å…¥æˆåŠŸåï¼Œå†é‡å‘½å
    os.replace(temp_file, progress_file)


def load_progress(progress_file: str) -> tuple[List[Dict], Dict, float, int]:
    """åŠ è½½æœç´¢è¿›åº¦"""
    if os.path.exists(progress_file):
        with open(progress_file, "r") as f:
            progress_data = json.load(f)
        return (
            progress_data.get("results", []),
            progress_data.get("best_params", {}),
            progress_data.get("best_score", 0.0),
            progress_data.get("completed_configs", 0),
        )
    return [], {}, 0.0, 0


def save_optimal_config(base_config: Dict[str, Any], best_params: Dict[str, Any], output_path: str):
    """ä¿å­˜ä¼˜åŒ–åçš„é…ç½®åˆ°TOMLæ–‡ä»¶"""
    if not best_params:
        return

    optimal_config = update_config(base_config, best_params)

    # æ·»åŠ æ³¨é‡Šåˆ°é…ç½®é¡¶éƒ¨
    optimal_config["_comment"] = "è‡ªåŠ¨è°ƒä¼˜ç”Ÿæˆçš„æœ€ä½³é…ç½®"

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # ä½¿ç”¨ tomli_w å†™å…¥ TOML æ–‡ä»¶
    with open(output_path, "wb") as f:
        tomli_w.dump(optimal_config, f)

    print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜ä¸º TOML æ ¼å¼: {output_path}")


def quick_search_params(
    data: np.ndarray,
    labels: np.ndarray,
    base_config: Dict[str, Any],
    output_path: str,
    progress_file: str = "quick_tune_progress.json",
) -> tuple[Dict[str, Any], float]:
    """å¿«é€Ÿæœç´¢é‡è¦å‚æ•°ï¼ˆæ¯ä¸ªé…ç½®éƒ½å®æ—¶ä¿å­˜ï¼‰"""

    # é‡è¦å‚æ•°çš„æœç´¢ç©ºé—´
    param_grid = {
        "anomaly_detection.scale_score_lambda": [0.15, 0.2, 0.25, 0.3],
        "anomaly_detection.anomaly_ratio": [2.0, 2.5, 3.0],
        "anomaly_detection.aggregation_method": ["max", "weighted_max"],
        "model.CFM.dropout": [0.1, 0.15, 0.2],
        "training.learning_rate": [0.0005, 0.001],
    }

    total_configs = int(np.prod([len(v) for v in param_grid.values()]))

    # å°è¯•åŠ è½½ä¹‹å‰çš„è¿›åº¦
    results, best_params, best_score, completed_configs = load_progress(progress_file)

    if completed_configs > 0:
        print(f"ğŸ”„ æ£€æµ‹åˆ°ä¹‹å‰çš„è¿›åº¦ï¼Œå·²å®Œæˆ {completed_configs}/{total_configs} ä¸ªé…ç½®")
        print(f"ğŸ“Š å½“å‰æœ€ä½³åˆ†æ•°: {best_score:.4f}")
    else:
        print(f"ğŸ†• å¼€å§‹å¿«é€Ÿæœç´¢ï¼Œæ€»è®¡é…ç½®æ•°: {total_configs}")

    param_names = list(param_grid.keys())
    param_values = list(param_grid.values())
    all_combinations = list(itertools.product(*param_values))

    for i, combination in enumerate(all_combinations):
        # è·³è¿‡å·²å®Œæˆçš„é…ç½®
        if i < completed_configs:
            continue

        param_updates = dict(zip(param_names, combination))
        config = update_config(base_config, param_updates)
        score = evaluate_config(data, labels, config)

        results.append({"params": param_updates, "score": score})

        if score is not None and score > best_score:
            best_score = score
            best_params = param_updates
            print(f"ğŸ‰ å‘ç°æ›´å¥½é…ç½®! F1={score:.4f}")
            # ç«‹å³ä¿å­˜æ–°çš„æœ€ä½³é…ç½®
            save_optimal_config(base_config, best_params, output_path)

        print(f"é…ç½® {i+1}/{total_configs}: F1={score:.4f} | å½“å‰æœ€ä½³: {best_score:.4f}")

        # æ¯æ¬¡éƒ½ä¿å­˜è¿›åº¦
        save_progress(progress_file, results, best_params, best_score)

    return best_params, best_score


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("-c", "--config", type=str, required=True, help="åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("-d", "--dataset", type=str, required=True, help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        default="configs/find_anomalies/ASD1/swift_optimized.toml",
        help="è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument("--resume", action="store_true", help="ä»ä¹‹å‰çš„è¿›åº¦ç»§ç»­æœç´¢")

    args = parser.parse_args()

    set_seed(1037)

    # åŠ è½½æ•°æ®å’Œé…ç½®
    print("ğŸš€ å¼€å§‹SWIFTå‚æ•°è‡ªåŠ¨è°ƒä¼˜...")

    base_config = load_config(args.config)
    df = pd.read_csv(args.dataset)
    data = df.iloc[:, :-1].values
    labels = df.iloc[:, -1].to_numpy()

    # print(f"æ•°æ®å½¢çŠ¶: {data.shape}, å¼‚å¸¸ç‚¹æ•°é‡: {np.sum(labels)}")

    # è®¾ç½®è¿›åº¦æ–‡ä»¶å
    progress_file = "tune_params_progress.json"

    if not args.resume and os.path.exists(progress_file):
        print(f"âš ï¸ å‘ç°ä¹‹å‰çš„è¿›åº¦æ–‡ä»¶ {progress_file}")
        response = input("æ˜¯å¦ç»§ç»­ä¹‹å‰çš„æœç´¢? (y/n): ").lower().strip()
        if response != "y":
            os.remove(progress_file)
            print("å·²åˆ é™¤ä¹‹å‰çš„è¿›åº¦æ–‡ä»¶ï¼Œå¼€å§‹æ–°çš„æœç´¢")

    print("æ‰§è¡Œå¿«é€Ÿå‚æ•°æœç´¢...")
    best_params, best_score = quick_search_params(data, labels, base_config, args.output, progress_file)

    print(f"\nğŸ‰ æœç´¢å®Œæˆ!")
    print(f"æœ€ä½³F1åˆ†æ•°: {best_score:.4f}")
    print(f"æœ€ä½³å‚æ•°: {best_params}")

    if best_params:
        print(f"âœ… æœ€ç»ˆé…ç½®å·²ä¿å­˜åˆ°: {args.output}")
        # æ¸…ç†è¿›åº¦æ–‡ä»¶
        if os.path.exists(progress_file):
            os.remove(progress_file)
            print(f"ğŸ§¹ å·²æ¸…ç†è¿›åº¦æ–‡ä»¶: {progress_file}")
    else:
        print("âŒ æœªæ‰¾åˆ°æ›´å¥½çš„å‚æ•°é…ç½®")
