import argparse
import copy
import gc
import os
from typing import Any, Dict, List, Union

import numpy as np
import optuna
import pandas as pd
import tomli
import tomli_w
import torch
from baselines.swift.swift_pipeline import swift_find_anomalies
from evaluation.metrics.anomaly_detection_metrics_label import affiliation_f
from tools.tools import set_seed


def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()


# ========== ç»Ÿä¸€å‚æ•°é…ç½® ==========
PARAM_CONFIG = {
    # æ•°æ®å¤„ç†å‚æ•° - è°ƒæ•´ä¸ºæ›´ä¿å®ˆçš„èŒƒå›´
    "seq_len": {
        "type": "categorical",
        "choices": [64, 128],
        "config_path": "data.seq_len",
    },
    "patch_size": {"type": "categorical", "choices": [8, 16, 32], "config_path": "data.patch_size"},
    "patch_stride": {
        "type": "categorical",
        "choices": [4, 8, 16],
        "config_path": "data.patch_stride",
    },
    # æ¨¡å‹å‚æ•° - è°ƒæ•´ä¸ºæ›´ä¿å®ˆçš„èŒƒå›´
    "d_cf": {
        "type": "categorical",
        "choices": [32, 64, 96],
        "config_path": "model.CFM.d_cf",
    },
    "d_model": {
        "type": "categorical",
        "choices": [64, 96, 128],
        "config_path": "model.CFM.d_model",
    },
    "cfm_num_heads": {
        "type": "categorical",
        "choices": [2, 4],
        "config_path": "model.CFM.num_heads",
    },
    "cfm_attention_dropout": {
        "type": "float",
        "low": 0.1,
        "high": 0.2,
        "config_path": "model.CFM.attention_dropout",
    },
    "cfm_num_layers": {
        "type": "int",
        "low": 3,
        "high": 5,  # ä»6å‡å°‘åˆ°5
        "config_path": "model.CFM.num_layers",
    },
    "cfm_dropout": {
        "type": "float",
        "low": 0.1,
        "high": 0.2,
        "config_path": "model.CFM.dropout",
    },
    "cfm_num_gat_heads": {
        "type": "categorical",
        "choices": [2, 4],  # ç§»é™¤8ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        "config_path": "model.CFM.num_gat_heads",
    },
    "cfm_gat_head_dim": {
        "type": "categorical",
        "choices": [8, 16],  # ç§»é™¤32ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        "config_path": "model.CFM.gat_head_dim",
    },
    # é¢‘åŸŸåˆ†è§£å‚æ•°
    "fm_level": {"type": "int", "low": 2, "high": 4, "config_path": "model.FM.level"},  # ä»5å‡å°‘åˆ°4
    "fm_wavelet": {
        "type": "categorical",
        "choices": ["db4", "sym4", "coif2"],
        "config_path": "model.FM.wavelet",
    },
    # æ—¶åºé‡æ„å‚æ•°
    "tsrm_is_flatten_individual": {
        "type": "categorical",
        "choices": [True, False],
        "config_path": "model.TSRM.is_flatten_individual",
    },
    # æŸå¤±å‡½æ•°å‚æ•°
    "ccd_loss_lambda": {
        "type": "float",
        "low": 1e-4,
        "high": 1e-1,
        "log": True,
        "config_path": "loss.ccd_loss_lambda",
    },
    "scale_loss_lambda": {"type": "float", "low": 0.2, "high": 1.0, "config_path": "loss.scale_loss_lambda"},
    "ccd_regular_lambda": {
        "type": "float",
        "low": 0.05,
        "high": 0.5,
        "config_path": "loss.ccd_regular_lambda",
    },
    "ccd_align_lambda": {"type": "float", "low": 0.5, "high": 2.0, "config_path": "loss.ccd_align_lambda"},
    # è®­ç»ƒå‚æ•°
    "learning_rate": {
        "type": "float",
        "low": 1e-4,
        "high": 5e-3,
        "log": True,
        "config_path": "training.learning_rate",
    },
    # å¼‚å¸¸æ£€æµ‹å‚æ•°
    "scale_score_lambda": {
        "type": "float",
        "low": 0.1,
        "high": 1.0,
        "config_path": "anomaly_detection.scale_score_lambda",
    },
}


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½ toml é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, "rb") as f:
            return tomli.load(f)
    except FileNotFoundError:
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    except Exception as e:
        raise ValueError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")


def update_config(base_config: Dict[str, Any], param_updates: Dict[str, Any]) -> Dict[str, Any]:
    """æ›´æ–° toml é…ç½®æ–‡ä»¶"""
    config = copy.deepcopy(base_config)
    for key_path, value in param_updates.items():
        # æ”¯æŒä½¿ç”¨è¯¸å¦‚ "model.CFM.d_cf" è¿™æ ·çš„ç‚¹è¯­æ³•æ›´æ–°åµŒå¥—å­—æ®µ
        keys = key_path.split(".")
        current = config
        # ä¾æ¬¡å‘ä¸‹æŸ¥æ‰¾/åˆ›å»ºåµŒå¥—å­—å…¸
        for key in keys[:-1]:
            if key not in current or not isinstance(current[key], dict):
                current[key] = {}
            current = current[key]
        # æ›´æ–°æœ€ç»ˆé”®å¯¹åº”çš„å€¼
        current[keys[-1]] = value
    return config


def save_config(config: Dict[str, Any], output_path: str):
    """ä¿å­˜ toml é…ç½®æ–‡ä»¶"""
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "wb") as f:
            tomli_w.dump(config, f)
        print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°: {output_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜é…ç½®å¤±è´¥: {e}")


def suggest_parameter(trial: optuna.trial.Trial, param_name: str, param_config: Dict[str, Any]) -> Any:
    """æ ¹æ®å‚æ•°é…ç½®åŠ¨æ€å»ºè®®å‚æ•°å€¼"""
    param_type = param_config["type"]

    if param_type == "categorical":
        return trial.suggest_categorical(param_name, param_config["choices"])
    elif param_type == "int":
        return trial.suggest_int(param_name, param_config["low"], param_config["high"])
    elif param_type == "float":
        log = param_config.get("log", False)
        return trial.suggest_float(param_name, param_config["low"], param_config["high"], log=log)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å‚æ•°ç±»å‹: {param_type}")


def validate_params(params: Dict[str, Any]) -> bool:
    """éªŒè¯å‚æ•°ç»„åˆçš„æœ‰æ•ˆæ€§"""
    seq_len = params["seq_len"]
    patch_size = params["patch_size"]
    patch_stride = params["patch_stride"]
    d_model = params["d_model"]
    d_cf = params["d_cf"]

    # åŸºæœ¬çº¦æŸæ£€æŸ¥
    if patch_size >= seq_len:
        return False
    if (seq_len - patch_size) % patch_stride != 0:
        return False
    if patch_stride > patch_size:
        return False
    if d_model < d_cf:
        return False

    return True


def objective(
    trial: optuna.trial.Trial, base_config: Dict[str, Any], data: np.ndarray, labels: np.ndarray
) -> float:
    """Optunaç›®æ ‡å‡½æ•°"""

    # åœ¨æ¯æ¬¡è¯•éªŒå¼€å§‹å‰æ¸…ç†GPUå†…å­˜
    clear_gpu_memory()

    # åŠ¨æ€ç”Ÿæˆæ‰€æœ‰å‚æ•°
    params = {}
    for param_name, param_config in PARAM_CONFIG.items():
        params[param_name] = suggest_parameter(trial, param_name, param_config)

    # éªŒè¯å‚æ•°ç»„åˆ
    if not validate_params(params):
        raise optuna.TrialPruned("æ— æ•ˆçš„å‚æ•°ç»„åˆ")

    # æ„å»ºå‚æ•°æ›´æ–°å­—å…¸
    param_updates = {}
    for param_name, value in params.items():
        config_path = PARAM_CONFIG[param_name]["config_path"]
        param_updates[config_path] = value

    # æ›´æ–°é…ç½®
    current_config = update_config(base_config, param_updates)
    # è®¾ç½® trial_number ç”¨äºæ—¥å¿—è®°å½•
    current_config["trial_number"] = trial.number

    try:
        # è¿è¡Œå¼‚å¸¸æ£€æµ‹
        predictions = swift_find_anomalies(data=data, config=current_config)
        score = affiliation_f(labels, predictions)

        # éªŒè¯åˆ†æ•°çš„æœ‰æ•ˆæ€§
        if not (0 <= score <= 1):
            print(f"âš ï¸ Trial {trial.number}: å¼‚å¸¸åˆ†æ•° {score:.4f}ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")

        # æ¸…ç†å†…å­˜
        clear_gpu_memory()

        return score

    except torch.cuda.OutOfMemoryError as e:
        print(f"ğŸ’¥ Trial {trial.number} GPUå†…å­˜ä¸è¶³: {str(e)[:100]}...")
        clear_gpu_memory()
        raise optuna.TrialPruned(f"GPUå†…å­˜ä¸è¶³: {e}")
    except Exception as e:
        print(f"âŒ Trial {trial.number} æ‰§è¡Œå¤±è´¥: {e}")
        clear_gpu_memory()
        raise optuna.TrialPruned(f"è¯•éªŒæ‰§è¡Œå¤±è´¥: {e}")


def run_optimization(args: argparse.Namespace):
    """è¿è¡Œ Optuna è¶…å‚æ•°ä¼˜åŒ–æµç¨‹"""
    print("ğŸš€ å¼€å§‹è¶…å‚æ•°è°ƒä¼˜...")
    print(f"ğŸ¯ ä»»åŠ¡: {args.task_name} | æ•°æ®é›†: {args.dataset_name} | ç®—æ³•: {args.algorithm_name}")
    print(f"ğŸ”¬ å¼‚å¸¸ç‡: {args.anomaly_ratio}% | è¯•éªŒæ¬¡æ•°: {args.n_trials}")

    # ---------- åŠ è½½åŸºç¡€é…ç½®ä¸æ•°æ® ----------
    try:
        base_config = load_config(args.base_config)
        # å°†å‘½ä»¤è¡ŒæŒ‡å®šçš„å¼‚å¸¸ç‡å†™å›é…ç½®ï¼Œç”¨äº swift_find_anomalies å†…éƒ¨é˜ˆå€¼è®¡ç®—
        if "anomaly_detection" in base_config and isinstance(base_config["anomaly_detection"], dict):
            base_config["anomaly_detection"]["anomaly_ratio"] = args.anomaly_ratio

        df = pd.read_csv(args.dataset_path)
        data = df.iloc[:, :-1].values
        labels = df.iloc[:, -1].to_numpy()
        print(f"ğŸ“ æ•°æ®å½¢çŠ¶: {data.shape}, æ ‡ç­¾å½¢çŠ¶: {labels.shape}")

        # æ•°æ®è´¨é‡æ£€æŸ¥
        if data.shape[0] != labels.shape[0]:
            raise ValueError(f"æ•°æ®å’Œæ ‡ç­¾é•¿åº¦ä¸åŒ¹é…: {data.shape[0]} vs {labels.shape[0]}")
        if len(np.unique(labels)) != 2:
            raise ValueError(f"æ ‡ç­¾åº”ä¸ºäºŒåˆ†ç±»ï¼Œä½†å‘ç° {len(np.unique(labels))} ä¸ªç±»åˆ«")

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # ---------- åˆ›å»º Optuna Study ----------
    study = optuna.create_study(
        direction="maximize",
        sampler=optuna.samplers.TPESampler(seed=1037),
        pruner=optuna.pruners.MedianPruner(),
    )

    # ---------- å›è°ƒç”¨äºè¿½è¸ªæœ€ä½³ç»“æœ ----------
    best_value: float = -1.0
    best_params: Dict[str, Any] = {}

    def _callback(study: optuna.Study, trial: optuna.trial.FrozenTrial):
        nonlocal best_value, best_params
        if (
            trial.state == optuna.trial.TrialState.COMPLETE
            and trial.value is not None
            and trial.value > best_value
        ):
            best_value = trial.value
            best_params = trial.params.copy()
            print(f"ğŸ‰ å‘ç°æ–°çš„æœ€ä½³ç»“æœ: F1 = {best_value:.4f} (Trial {trial.number})")

            param_updates = {}
            for param_name, value in trial.params.items():
                if param_name in PARAM_CONFIG:
                    config_path = PARAM_CONFIG[param_name]["config_path"]
                    param_updates[config_path] = value

            current_best_config = update_config(base_config, param_updates)
            save_config(current_best_config, args.output_config)

    # ---------- æ‰§è¡Œä¼˜åŒ– ----------
    try:
        study.optimize(
            lambda t: objective(t, base_config, data, labels),
            n_trials=args.n_trials,
            callbacks=[_callback],
        )
    except KeyboardInterrupt:
        print("â¹ï¸ ä¼˜åŒ–è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {e}")

    # ---------- æœ€ç»ˆç»“æœæ€»ç»“ ----------
    if best_params:
        print("\nâœ… è°ƒä¼˜å®Œæˆ!")
        print(f"ğŸ“Š æœ€ä½³ F1 åˆ†æ•°: {best_value:.4f}")
        print(f"ğŸ† æœ€ä½³å‚æ•°ç»„åˆ: {dict(list(best_params.items())[:5])}...")  # æ˜¾ç¤ºå‰5ä¸ªå‚æ•°ä½œä¸ºé¢„è§ˆ
    else:
        print("\nâš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æœ€ä½³å‚æ•°ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œé…ç½®")


# --------------------------- CLI ---------------------------
if __name__ == "__main__":
    cli_parser = argparse.ArgumentParser(description="SWIFT è¶…å‚æ•°è°ƒä¼˜å·¥å…·")

    # åŸºæœ¬å‚æ•°
    cli_parser.add_argument("--task-name", type=str, required=True, help="ä»»åŠ¡åç§°")
    cli_parser.add_argument("--dataset-name", type=str, required=True, help="æ•°æ®é›†åç§°")
    cli_parser.add_argument("--algorithm-name", type=str, required=True, help="ç®—æ³•åç§°")
    cli_parser.add_argument("--anomaly-ratio", type=float, required=True, help="å¼‚å¸¸ç‡ (ç™¾åˆ†æ¯”)")
    cli_parser.add_argument("--n-trials", type=int, default=100, help="Optuna è¯•éªŒæ¬¡æ•°")

    # è·¯å¾„å‚æ•°
    cli_parser.add_argument(
        "--base-config",
        type=str,
        default="configs/{task_name}/{dataset_name}/{algorithm_name}.toml",
        help="åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    cli_parser.add_argument(
        "--output-config",
        type=str,
        default="configs/{task_name}/{dataset_name}/{algorithm_name}_best_ar_{ratio}.toml",
        help="è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„",
    )
    cli_parser.add_argument(
        "--dataset-path",
        type=str,
        default="datasets/{dataset_name}.csv",
        help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„",
    )

    _args = cli_parser.parse_args()

    # è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
    set_seed(1037)

    # ---------- æ ¼å¼åŒ–å ä½ç¬¦ ----------
    _args.base_config = _args.base_config.format(
        task_name=_args.task_name,
        dataset_name=_args.dataset_name,
        algorithm_name=_args.algorithm_name,
    )
    _args.output_config = _args.output_config.format(
        task_name=_args.task_name,
        dataset_name=_args.dataset_name,
        algorithm_name=_args.algorithm_name,
        ratio=_args.anomaly_ratio,
    )
    _args.dataset_path = _args.dataset_path.format(dataset_name=_args.dataset_name)

    run_optimization(_args)
