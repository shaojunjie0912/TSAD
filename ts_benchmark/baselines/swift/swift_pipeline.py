from typing import Any, Dict, Literal, Optional

import numpy as np
import torch
import torch.nn as nn
from torch.optim import lr_scheduler

from .model.swift import SWIFT
from .utils.training import EarlyStopping, get_dataloader


class SWIFTPipeline(nn.Module):
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.config = config
        self.data_config = config["data"]
        self.model_config = config["model"]
        self.training_config = config["training"]
        self.loss_config = config["loss"]

        self.time_loss_fn = nn.HuberLoss(delta=self.loss_config["time_loss_delta"])  # æ—¶åŸŸæŸå¤±å‡½æ•°
        self.scale_loss_fn = nn.HuberLoss(delta=self.loss_config["scale_loss_delta"])  # å°ºåº¦åŸŸæŸå¤±å‡½æ•°

        self.ccd_loss_lambda = self.loss_config["ccd_loss_lambda"]
        self.scale_loss_lambda = self.loss_config["scale_loss_lambda"]

        self.anomaly_config = config["anomaly_detection"]
        self.scale_score_lambda = self.anomaly_config["scale_score_lambda"]
        self.anomaly_ratio: float = self.anomaly_config["anomaly_ratio"]

        self.batch_size: int = self.training_config["batch_size"]
        self.seq_len: int = self.data_config["seq_len"]

        # anomaly detection  # NOTE: ä¿ç•™æ‰€æœ‰ä½ç½® square error
        # TODO: æ—¶é—´åŸŸ + å°ºåº¦åŸŸå¼‚å¸¸è¯„åˆ†æ ‡å‡†
        self.time_anomaly_criterion = nn.MSELoss(reduction="none")
        self.scale_anomaly_criterion = nn.MSELoss(reduction="none")

        self.fitted: bool = False

        # éªŒè¯é›†åˆ†æ•°ç¼“å­˜
        self.validation_scores: Optional[np.ndarray] = None
        self.val_data: Optional[np.ndarray] = None

    # train + val
    def fit(self, data: np.ndarray):
        train_ratio = self.data_config["train_ratio"]
        len_train = int(len(data) * train_ratio)

        train_data = data[:len_train]
        val_data = data[len_train:]

        # NOTE: å°†åŽŸå§‹éªŒè¯é›†ä¿å­˜ä¸ºå®žä¾‹å±žæ€§
        self.val_data = val_data

        self.train_dataloader = get_dataloader(
            stage="train",
            data=train_data,
            batch_size=self.batch_size,
            window_size=self.seq_len,
            step_size=1,
            shuffle=True,
            transform=None,
            target_transform=None,
        )

        self.val_dataloader = get_dataloader(
            stage="val",
            data=val_data,
            batch_size=self.batch_size,
            window_size=self.seq_len,
            step_size=1,
            shuffle=False,
            transform=None,
            target_transform=None,
        )

        fm_config = self.model_config["FM"]
        cfm_config = self.model_config["CFM"]
        tsrm_config = self.model_config["TSRM"]

        self.model = SWIFT(
            # data config
            num_features=data.shape[1],
            seq_len=self.data_config["seq_len"],
            patch_size=self.data_config["patch_size"],
            patch_stride=self.data_config["patch_stride"],
            # model config
            affine=fm_config["affine"],
            subtract_last=fm_config["subtract_last"],
            level=fm_config["level"],
            wavelet=fm_config["wavelet"],
            mode=fm_config["mode"],
            num_layers=cfm_config["num_layers"],
            dim=cfm_config["d_cf"],
            d_model=cfm_config["d_model"],
            num_heads=cfm_config["num_heads"],
            d_head=cfm_config["d_head"],
            d_ff=cfm_config["d_ff"],
            dropout=cfm_config["dropout"],
            attention_dropout=cfm_config["attention_dropout"],
            num_gat_heads=cfm_config["num_gat_heads"],
            gat_head_dim=cfm_config["gat_head_dim"],
            gat_dropout_rate=cfm_config["gat_dropout_rate"],
            is_flatten_individual=tsrm_config["is_flatten_individual"],
            rec_head_dropout=tsrm_config["rec_head_dropout"],
            # loss config
            ccd_regular_lambda=self.loss_config["ccd_regular_lambda"],
            ccd_align_lambda=self.loss_config["ccd_align_lambda"],
            ccd_align_temperature=self.loss_config["ccd_align_temperature"],
        )
        self.model.to(self.device)

        self.early_stopping = EarlyStopping(
            patience=self.training_config["es_patience"],
            delta=self.training_config["es_delta"],
            mode="min",
            verbose=True,
        )

        train_steps = len(self.train_dataloader)

        # NOTE: ä¸€ä¸ªä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.training_config["learning_rate"])

        # NOTE: ä¸€ä¸ªè°ƒåº¦å™¨
        self.scheduler = lr_scheduler.OneCycleLR(
            optimizer=self.optimizer,
            steps_per_epoch=train_steps,
            epochs=self.training_config["num_epochs"],
            pct_start=self.training_config["pct_start"],
            max_lr=self.training_config["learning_rate"],
        )

        # ----------------------------------------
        # ----------------- è®­ç»ƒ -----------------
        # ----------------------------------------
        for epoch_idx in range(self.training_config["num_epochs"]):
            self.model.train()
            train_loss = []

            for i, (x, _) in enumerate(self.train_dataloader):
                self.optimizer.zero_grad()  # æ¢¯åº¦æ¸…é›¶

                x = x.float().to(self.device)

                x_orig, x_hat, s, s_hat, ccd_loss = self.model(x)

                # --- è®¡ç®—æ€»æŸå¤± ---
                time_rec_loss = self.time_loss_fn(x_hat, x_orig)
                scale_rec_loss = self.scale_loss_fn(s_hat, s)
                loss = (
                    time_rec_loss + self.scale_loss_lambda * scale_rec_loss + self.ccd_loss_lambda * ccd_loss
                )

                # # ---- æ·»åŠ è¿™è¡Œç”¨äºŽè¯Šæ–­ ----
                # if i % 100 == 0:  # æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡
                #     print(
                #         f"batch {i}: time_loss={time_rec_loss.item():.4f}, "
                #         f"scale_loss={scale_rec_loss.item():.4f}, "
                #         f"ccd_loss={ccd_loss.item():.4f}"
                #     )

                train_loss.append(loss.item())

                # --- åå‘ä¼ æ’­ä¸Žæ›´æ–° ---
                loss.backward()
                self.optimizer.step()

                # åœ¨æ¯ä¸ª batch åŽæ›´æ–°å­¦ä¹ çŽ‡
                self.scheduler.step()

            # --- Epoch ç»“æŸåŽçš„éªŒè¯ä¸Žæ‰“å° ---
            train_loss_avg = np.mean(train_loss)
            valid_loss = self.validate(self.val_dataloader, self.time_loss_fn)
            print(
                f"Epoch [{epoch_idx+1}/{self.training_config['num_epochs']}], "
                f"Train Loss: {train_loss_avg:.6f}, Valid Loss: {valid_loss:.6f}, "
                f"LR: {self.optimizer.param_groups[0]['lr']:.6f}"
            )

            self.early_stopping(float(valid_loss), self.model)

            if self.early_stopping.should_stop:
                print("Early stopping triggered. Loading best model weights.")
                # åœ¨ä¸­æ–­å¾ªçŽ¯å‰ï¼Œè‡ªåŠ¨åŠ è½½æ€§èƒ½æœ€ä½³çš„æ¨¡åž‹æƒé‡
                self.early_stopping.load_best_weights(self.model)
                break

        self.fitted = True
        # åœ¨è®­ç»ƒç»“æŸåŽ, è®¡ç®—å¹¶ç¼“å­˜éªŒè¯é›†åˆ†æ•°ç”¨äºŽé˜ˆå€¼è®¡ç®—
        # æ³¨æ„ï¼šéªŒè¯é›†å¯èƒ½åŒ…å«å°‘é‡å¼‚å¸¸ï¼Œä½†è¿™æ˜¯å¼‚å¸¸æ£€æµ‹çš„æ­£å¸¸æƒ…å†µ
        print("\nCalculating and caching validation scores for threshold calculation...")
        self.model.eval()
        if self.val_data is not None:
            self.validation_scores = self.score_anomalies(self.val_data)
        self.model.train()
        # print("Fitting process complete. Validation scores are cached for threshold calculation.")

    def validate(self, val_dataloader, loss_fn):
        self.model.eval()  # -> eval
        total_loss = []
        with torch.no_grad():
            for x, _ in val_dataloader:
                x = x.float().to(self.device)
                x_orig, x_hat, s_orig, s_hat, _ = self.model(x)
                time_rec_loss = loss_fn(x_hat, x_orig)
                scale_rec_loss = loss_fn(s_hat, s_orig)
                loss = time_rec_loss + self.scale_loss_lambda * scale_rec_loss
                total_loss.append(loss.item())
        self.model.train()  # -> train
        return np.mean(total_loss)

    def _calculate_threshold(
        self,
        val_scores: np.ndarray,
        strategy: Literal["percentile", "robust_percentile", "std", "adaptive"] = "adaptive",
        anomaly_ratio: Optional[float] = None,
        **kwargs,
    ) -> float:
        """æ ¹æ®ä¸åŒç­–ç•¥è®¡ç®—å¼‚å¸¸é˜ˆå€¼

        ä½¿ç”¨éªŒè¯é›†çš„å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒæ¥è®¡ç®—é˜ˆå€¼ã€‚
        æ³¨æ„ï¼šéªŒè¯é›†å¯èƒ½åŒ…å«å°‘é‡å¼‚å¸¸ï¼Œä½†å¼‚å¸¸çŽ‡è¾ƒå°ï¼Œ
        æ¨¡åž‹éœ€è¦ä»Žå¤§éƒ¨åˆ†æ­£å¸¸æ•°æ®ä¸­å­¦ä¹ å¹¶è®¾å®šåˆç†çš„é˜ˆå€¼ã€‚

        Args:
            val_scores: éªŒè¯é›†å¼‚å¸¸åˆ†æ•°ï¼ˆå¯èƒ½åŒ…å«å°‘é‡å¼‚å¸¸ï¼‰
            strategy: é˜ˆå€¼è®¡ç®—ç­–ç•¥
            anomaly_ratio: å¼‚å¸¸æ¯”ä¾‹ï¼Œå¦‚æžœæä¾›åˆ™è¦†ç›–é»˜è®¤é…ç½®
            **kwargs: å…¶ä»–å‚æ•°
        """
        if anomaly_ratio is None:
            anomaly_ratio = self.anomaly_ratio

        # print(f"Calculating threshold using '{strategy}' strategy with anomaly_ratio={anomaly_ratio:.3f}...")

        if strategy == "percentile":
            print("Threshold strategy: percentile")
            # ç™¾åˆ†ä½æ•°ç­–ç•¥
            threshold = np.percentile(val_scores, 100 - anomaly_ratio)

        elif strategy == "robust_percentile":
            print("Threshold strategy: robust_percentile")
            # æ”¹è¿›çš„é²æ£’ç™¾åˆ†ä½æ•°ç­–ç•¥
            q_robust = kwargs.get("q_robust", 95.0)
            p_robust = kwargs.get("p_robust", 80.0)

            tail_threshold = np.percentile(val_scores, q_robust)
            tail_scores = val_scores[val_scores > tail_threshold]

            if len(tail_scores) == 0:
                print(f"  Warning: No scores above {q_robust}th percentile. Using percentile fallback.")
                return float(np.percentile(val_scores, 100 - anomaly_ratio))

            final_threshold = np.percentile(tail_scores, p_robust)
            # print(f"  Robust params: q={q_robust}, p={p_robust}")
            threshold = final_threshold

        elif strategy == "std":
            print("Threshold strategy: std")
            # æ ‡å‡†å·®ç­–ç•¥
            n_std = kwargs.get("n_std", 2.5)  # é™ä½Žä»Ž3.0åˆ°2.5ï¼Œæ›´æ•æ„Ÿ
            mean = np.mean(val_scores)
            std = np.std(val_scores)
            threshold = mean + n_std * std
            # print(f"  STD params: mean={mean:.4f}, std={std:.4f}, n_std={n_std}")

        elif strategy == "adaptive":
            print("Threshold strategy: adaptive")
            # æ–°å¢žï¼šè‡ªé€‚åº”é˜ˆå€¼ç­–ç•¥
            # ç»“åˆå¤šç§æ–¹æ³•ï¼Œæ ¹æ®æ•°æ®åˆ†å¸ƒç‰¹å¾é€‰æ‹©æœ€ä¼˜ç­–ç•¥

            # è®¡ç®—åˆ†æ•°çš„ç»Ÿè®¡ç‰¹å¾
            mean_score = np.mean(val_scores)
            std_score = np.std(val_scores)
            skewness = self._calculate_skewness(val_scores)

            # æ ¹æ®ååº¦é€‰æ‹©ç­–ç•¥
            if abs(skewness) > 1.5:  # é«˜ååº¦ï¼Œä½¿ç”¨é²æ£’æ–¹æ³•
                # print(f"  High skewness detected ({skewness:.3f}), using robust method...")
                q_robust = 92.0 + min(3.0, float(abs(skewness)))  # åŠ¨æ€è°ƒæ•´
                tail_threshold = np.percentile(val_scores, q_robust)
                tail_scores = val_scores[val_scores > tail_threshold]

                if len(tail_scores) > 0:
                    threshold = np.percentile(tail_scores, 75.0)
                else:
                    threshold = np.percentile(val_scores, 100 - anomaly_ratio)
            else:  # ä½Žååº¦ï¼Œä½¿ç”¨æ”¹è¿›çš„ç™¾åˆ†ä½æ•°æ–¹æ³•
                # print(f"  Normal distribution detected (skewness={skewness:.3f}), using percentile method...")
                # ä½¿ç”¨æ›´ä¿å®ˆçš„ç™¾åˆ†ä½æ•°
                base_percentile = 100 - anomaly_ratio
                # æ ¹æ®æ ‡å‡†å·®è°ƒæ•´
                cv = std_score / (mean_score + 1e-8)  # å˜å¼‚ç³»æ•°
                adjusted_percentile = base_percentile - min(2.0, float(cv * 10))  # åŠ¨æ€è°ƒæ•´
                threshold = np.percentile(val_scores, max(90.0, adjusted_percentile))

            # print(f"  Adaptive params: skewness={skewness:.3f}, final_threshold={threshold:.6f}")

        else:
            raise ValueError(f"Unknown threshold strategy: {strategy}")

        return float(threshold)

    def _calculate_skewness(self, data: np.ndarray) -> float:
        """è®¡ç®—æ•°æ®çš„ååº¦"""
        mean = np.mean(data)
        std = np.std(data)
        if std == 0:
            return 0.0
        return float(np.mean(((data - mean) / std) ** 3))

    def score_anomalies(
        self,
        data: np.ndarray,
        aggregation_method: Literal["mean", "max", "weighted_max"] = "weighted_max",
    ) -> np.ndarray:
        """æ”¹è¿›çš„å¼‚å¸¸åˆ†æ•°è®¡ç®—ï¼Œæ”¯æŒå¤šç§èšåˆæ–¹æ³•"""
        if not self.fitted:
            raise ValueError("Please fit the model first!")

        self.predict_dataloader = get_dataloader(
            stage="predict",
            data=data,
            batch_size=self.batch_size,
            window_size=self.seq_len,
            step_size=1,
            shuffle=False,
            transform=None,
            target_transform=None,
        )
        self.model.to(self.device)
        self.model.eval()

        anomaly_scores_sum = np.zeros(len(data))
        anomaly_scores_max = np.zeros(len(data))  # æ–°å¢žï¼šæœ€å¤§å€¼èšåˆ
        counts = np.zeros(len(data))

        with torch.no_grad():
            for i, (x, _, padding_mask, start_indices) in enumerate(self.predict_dataloader):
                x = x.float().to(self.device)
                padding_mask = padding_mask.float().to(self.device)
                x_orig, x_hat, s_orig, s_hat, _ = self.model(x)

                # è®¡ç®—æ—¶é—´åŸŸå’Œå°ºåº¦åŸŸåˆ†æ•°
                time_score = torch.mean(self.time_anomaly_criterion(x_hat, x_orig), dim=-1)
                scale_score = torch.mean(self.scale_anomaly_criterion(s_hat, s_orig), dim=-1)

                score = time_score + self.scale_score_lambda * scale_score
                score_np = score.cpu().numpy()
                padding_mask_np = padding_mask.cpu().numpy()

                for j in range(len(start_indices)):
                    start = start_indices[j]
                    end = start + self.seq_len
                    window_score = score_np[j]
                    window_mask = padding_mask_np[j]

                    actual_end = min(end, len(data))
                    valid_length = actual_end - start

                    # åŽŸæœ‰çš„æ±‚å’Œèšåˆ
                    anomaly_scores_sum[start:actual_end] += window_score[:valid_length]
                    # æ–°å¢žçš„æœ€å¤§å€¼èšåˆ
                    anomaly_scores_max[start:actual_end] = np.maximum(
                        anomaly_scores_max[start:actual_end], window_score[:valid_length]
                    )
                    counts[start:actual_end] += window_mask[:valid_length]

        counts[counts == 0] = 1

        if aggregation_method == "mean":
            print("Aggregation method: mean")
            final_scores = anomaly_scores_sum / counts
        elif aggregation_method == "max":
            print("Aggregation method: max")
            final_scores = anomaly_scores_max
        elif aggregation_method == "weighted_max":
            print("Aggregation method: weighted_max")
            # åŠ æƒæœ€å¤§å€¼ï¼šç»“åˆå¹³å‡å€¼å’Œæœ€å¤§å€¼
            mean_scores = anomaly_scores_sum / counts
            alpha = 0.3  # å¹³å‡å€¼æƒé‡
            beta = 0.7  # æœ€å¤§å€¼æƒé‡
            final_scores = alpha * mean_scores + beta * anomaly_scores_max
        else:
            raise ValueError(f"Unknown aggregation method: {aggregation_method}")

        return final_scores

    def find_anomalies(
        self,
        data: np.ndarray,
        threshold_strategy: Literal["percentile", "robust_percentile", "std", "adaptive"] = "adaptive",
        aggregation_method: Literal["mean", "max", "weighted_max"] = "weighted_max",
        **kwargs,
    ) -> tuple[np.ndarray, np.ndarray]:
        """SWIFTå¼‚å¸¸æ£€æµ‹å‡½æ•°

        Args:
            data: æµ‹è¯•æ•°æ®ï¼ˆé€šå¸¸æ˜¯å®Œæ•´çš„total_dataï¼ŒåŒ…å«å¼‚å¸¸ï¼‰
            threshold_strategy: é˜ˆå€¼è®¡ç®—ç­–ç•¥
            aggregation_method: åˆ†æ•°èšåˆæ–¹æ³•
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            predictions: å¼‚å¸¸é¢„æµ‹æ ‡ç­¾ (0: æ­£å¸¸, 1: å¼‚å¸¸)
            scores: å¼‚å¸¸åˆ†æ•°
        """
        if not self.fitted:
            raise ValueError("Please fit the model first!")

        # è®¡ç®—æµ‹è¯•æ•°æ®çš„å¼‚å¸¸åˆ†æ•°
        test_scores = self.score_anomalies(data, aggregation_method=aggregation_method)

        # ä½¿ç”¨éªŒè¯é›†è®¡ç®—é˜ˆå€¼ï¼ˆæ ‡å‡†åšæ³•ï¼‰
        if self.validation_scores is None:
            raise RuntimeError(
                "Validation scores were not cached. Please check the fit() method. "
                "Make sure the model was trained with validation data."
            )

        # print("ðŸ“ Using validation set scores for threshold calculation")
        # print(f"   Validation set size: {len(self.validation_scores)}")
        # print(
        #     f"   Validation score range: [{np.min(self.validation_scores):.4f}, {np.max(self.validation_scores):.4f}]"
        # )
        # print(f"   Test score range: [{np.min(test_scores):.4f}, {np.max(test_scores):.4f}]")

        # è®¡ç®—é˜ˆå€¼
        threshold = self._calculate_threshold(self.validation_scores, strategy=threshold_strategy, **kwargs)
        # print(f"ðŸŽ¯ Anomaly threshold determined: {threshold:.6f}")

        # ç”Ÿæˆé¢„æµ‹ç»“æžœ
        predictions = (test_scores > threshold).astype(int)

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        anomaly_count = np.sum(predictions)
        anomaly_rate = anomaly_count / len(predictions)
        # print(f"ðŸš¨ Detected {anomaly_count} anomalies out of {len(predictions)} points ({anomaly_rate:.3%})")

        # æä¾›éªŒè¯é›†çš„å‚è€ƒä¿¡æ¯
        val_anomaly_count = np.sum(self.validation_scores > threshold)
        val_anomaly_rate = val_anomaly_count / len(self.validation_scores)
        # print(
        #     f"ðŸ“Š For reference: {val_anomaly_count} points in validation set would be flagged as anomalies ({val_anomaly_rate:.3%})"
        # )

        return predictions, test_scores


def swift_score_anomalies(data: np.ndarray, config: Dict[str, Any]) -> np.ndarray:
    """
    è®¡ç®—å¼‚å¸¸åˆ†æ•°
    """
    pipeline = SWIFTPipeline(config)
    pipeline.fit(data)
    aggregation_method = config["anomaly_detection"]["aggregation_method"]
    scores = pipeline.score_anomalies(data, aggregation_method=aggregation_method)

    return scores


def swift_find_anomalies(data: np.ndarray, config: Dict[str, Any]) -> np.ndarray:
    """
    æ‰¾åˆ°å¼‚å¸¸ç‚¹
    """
    pipeline = SWIFTPipeline(config)
    pipeline.fit(data)
    predictions, scores = pipeline.find_anomalies(data)

    return predictions
